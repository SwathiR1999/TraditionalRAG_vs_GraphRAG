{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2D8gKYWu4Yx"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index transformers accelerate bitsandbytes sentence-transformers --quiet\n",
        "!pip install llama-index-llms-huggingface --quiet\n",
        "!pip install llama-index-embeddings-huggingface --quiet\n",
        "!pip install llama-index-graph-stores-neo4j --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Traditional RAG**"
      ],
      "metadata": {
        "id": "AUAuBHi7EghY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "0o0Io7ABvmYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-chat\""
      ],
      "metadata": {
        "id": "umlGThPsx3cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "4hWK5_TVx5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model_name,\n",
        "    model_name=model_name,\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 2048},\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")"
      ],
      "metadata": {
        "id": "oypEa_W4yl54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "5OgFbuDW0vXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "mc8WwT5yynhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.schema import Document\n",
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "documents = [\n",
        "    Document(text=\"A is daughter of B.\"),\n",
        "    Document(text=\"B is sister of C.\"),\n",
        "    Document(text=\"D is daughter of C.\")\n",
        "]\n",
        "\n",
        "vector_store = SimpleVectorStore()\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents,\n",
        "                                        embed_model=embed_model,\n",
        "                                        llm=llm,\n",
        "                                        storage_context=storage_context)\n",
        "query_engine = index.as_query_engine(\n",
        "    include_text=True,\n",
        "    response_mode=\"tree_summarize\",\n",
        "    embedding_mode=\"hybrid\",\n",
        "    similarity_top_k=5,\n",
        ")\n",
        "response = query_engine.query(\"Who is cousin of A?\")"
      ],
      "metadata": {
        "id": "dlC1gCubzwiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "Z0WxqeDVDmOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GraphRAG**"
      ],
      "metadata": {
        "id": "EfaUVzf64nlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ],
      "metadata": {
        "id": "278yg9yt4x9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.schema import Document\n",
        "\n",
        "documents = [\n",
        "    Document(text=\"A is daughter of B.\"),\n",
        "    Document(text=\"B is sister of C.\"),\n",
        "    Document(text=\"D is daughter of C.\")\n",
        "]"
      ],
      "metadata": {
        "id": "pKv8jSOC43_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "TajwlERs5Jy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"deepseek-ai/deepseek-llm-7b-chat\""
      ],
      "metadata": {
        "id": "vB-_yyLZ5Jy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "i1DveWVE5Jy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=2048,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=model_name,\n",
        "    model_name=model_name,\n",
        "    device_map=\"auto\",\n",
        "    tokenizer_kwargs={\"max_length\": 2048},\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        ")"
      ],
      "metadata": {
        "id": "lPmnHWjB5Jy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "Settings.llm = llm"
      ],
      "metadata": {
        "id": "GrfYjDfP5Jy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import StorageContext\n",
        "# from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "\n",
        "# graph_store = SimpleGraphStore()\n",
        "from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
        "\n",
        "username = \"XXXX\"\n",
        "password = \"XXXX\"\n",
        "url = \"XXXX\"\n",
        "database = \"neo4j\"\n",
        "graph_store = Neo4jGraphStore(\n",
        "    username=username,\n",
        "    password=password,\n",
        "    url=url,\n",
        "    database=database,\n",
        ")\n",
        "storage_context = StorageContext.from_defaults(graph_store=graph_store)"
      ],
      "metadata": {
        "id": "uCqQtmcx5RaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = KnowledgeGraphIndex.from_documents(\n",
        "    documents,\n",
        "    max_triplets_per_chunk=3,\n",
        "    include_embeddings=True,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,\n",
        "    llm=llm,\n",
        ")"
      ],
      "metadata": {
        "id": "8kyJLK1QApPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    include_text=True,\n",
        "    response_mode=\"tree_summarize\",\n",
        "    embedding_mode=\"hybrid\",\n",
        "    similarity_top_k=5,\n",
        ")\n",
        "response = query_engine.query(\n",
        "    \"Who is cousin of A?\",\n",
        ")"
      ],
      "metadata": {
        "id": "CGvwiRcsAzhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "kK32Pjy3BCmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}